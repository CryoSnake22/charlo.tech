<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         Master Theorem made easy
        
    </title><meta content="Master Theorem made easy" property=og:title><meta content="My personnal website" property=og:description><meta content="My personnal website" name=description><link href=/icon/favicon.png rel=icon type=image/png><link href=https://charlo.tech/fonts.css rel=stylesheet><script src=https://charlo.tech/js/codeblock.js></script><script src=https://charlo.tech/js/note.js></script><script>window.MathJax={loader:{load:['[tex]/ams']},tex:{packages:{'[+]':['ams']},inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,tags:'ams'}}</script><script defer id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link href=https://charlo.tech/public/main.css rel=stylesheet><link href=https://charlo.tech/rss.xml rel=alternate title=CharloPL type=application/rss+xml><link href=https://charlo.tech/theme/light.css rel=stylesheet><link href=https://charlo.tech/theme/dark.css id=darkModeStyle rel=stylesheet><script src=https://charlo.tech/js/themetoggle.js></script><script>setTheme(getSavedTheme())</script><link href=https://charlo.tech/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=https://charlo.tech/>CharloPL</a><div class=socials><a class=social href=https://github.com/CryoSnake22 rel=me> <img alt=github src=https://charlo.tech/social_icons/github.svg> </a><a class=social href rel=me> <img alt=youtube src=https://charlo.tech/social_icons/youtube.svg> </a><a class=social href rel=me> <img alt=rss src=https://charlo.tech/social_icons/rss.svg> </a><a class=social href=https://www.linkedin.com/in/charles-antoine-pouliot-977286283/ rel=me> <img alt=linkedin src=https://charlo.tech/social_icons/linkedin.svg> </a></div></div><nav><a href=https://charlo.tech/posts style=margin-left:.25em>/posts</a><a href=https://charlo.tech/projects style=margin-left:.25em>/projects</a><a href=https://charlo.tech/about style=margin-left:.25em>/about</a><a onclick="toggleTheme(); event.preventDefault();" href=# id=dark-mode-toggle> <img alt=Light id=sun-icon src=https://charlo.tech/feather/sun.svg style=filter:invert(1)> <img alt=Dark id=moon-icon src=https://charlo.tech/feather/moon.svg> </a><script>updateItemToggleTheme()</script></nav></header><main><article><div class=title><div class=page-header>Master Theorem made easy<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2025-09-16</time></div></div><section class=body><p><strong>Definition</strong> (Master Theorem)<p>Let $T(n)$ be the runtime of some algorithm of the form<p>$$ T(n)=aT\left( \frac{n}{b} \right)+f(n) $$<p>Where<ul><li>$a\geq1$ is the number of subproblems at each level<li>$b>1$ is the size of each sub problem.<li>$f(n)$ is the work done outside each recursive call.</ul><p>I am going to denote the total work done by the recursive portion as $n^{\log_{b}a }$. The reason for doing so is explained below and it is highly recommended to take the time to understand it. Now three cases arise:<p><em>Case 1:</em> $f(n)=O(n^{\log_{b} a-\varepsilon})$ for some $\varepsilon>0$<p>In this case, the non-recursive work grows <em>strictly slower</em> than the recursive work. The polynomial growth rate must be strictly lower which is why we have a $-\varepsilon$ term. This leads to a total work of<p>$$ T(n)=\Theta(n^{\log_{b}a}) $$<p>So just the recursive work.<p><em>Case 2:</em> $f(n)=\Theta(n^{\log_{b}a} \log^k n)$<p>In this case, both works grow at the same <strong>polynomial</strong> rate. The log term is added to allow for more expressivity in the wiggle room in the middle. This leads to a total work<p>$$ T(n)=\Theta(n^{\log_{b} a} (\log n)^{k+1}) $$<p><em>Case 3:</em> $f(n)=\Omega (n^{\log_{b} a+\varepsilon})$ for some $\varepsilon>0$<p>In this case, the non-recursive work grows polynomially strictly faster than the recursive work which leads to a run time of<p>$$ \Theta(f(n)) $$<p><strong>Work and Intuition</strong> (Master Theorem)<p>Let us work through the logic behind the theorem. It is important to note that $f(n)$ quantifies the work done outside the recursion, aka everything other than making a recursive call whilst $aT \left( \frac{n}{b} \right)$ quantifies the work done as a result of recursion. With this in mind let us notice the following fact:<ol><li>$a^{i}$ is the total number of subproblems for $i$ iterations</ol><p>We want to know how many iterations are needed to get a problem size of $c$ or to reach a base case. To do so we need to know when<p>$$ \begin{aligned} \frac{n}{b^{i}} & =c \\ \frac{n}{c} & = b^{i} \\ \log_{b} \frac{n}{c} & =i \end{aligned} $$<p>Which, by substitution leads to<p>$$ \begin{aligned} a^{i} & =a^{\log_{b} \frac{n}{c}} \\ & =\left( \frac{n}{c} \right) ^{\log_{b}a } \\ & =c^{-\log_{b}a}\cdot n^{\log_{b}a} \end{aligned} $$<p>And since we only care about asymptotic behaviour, we can ignore the $c^{-\log_{b}a}$ since $c$ is a constant. Which means that for all base cases of any sizes, the asymptotic number of leaves is<p>$$ \Theta(n^{\log_{b}a}) $$<p>Therefore the total number of iterations/work done as a result of our recursion is $\boxed{n^{\log_{b} a}}$. All that’s left to do now is to compare the recursive work vs non-recursive work. This is not formally stated as a limit test but it is what is fundamentally happening. The “Three cases” of the master theorem are really just about asking <em>Which grows faster, the recursive work or non-recursive work</em>. The logic behind cases 1 and 3 is pretty self explanatory, but case 2 can get a bit confusing so let us analyse it.<p><strong>Intuition behind case 2</strong>:<p>The master theorem concerns itself with differences in <em>polynomial growth</em> but if one was to simply check if $f(n)=\Theta(n^{\log_{b} a})$, this essentially ignores all cases where $f(n)$ is of the same polynomial growth rate as the recursive work but with polylog terms (terms of the form $\log^kx$). This can seem obtuse but essentially it gives enough resolution to actually compare and get all the possible runtimes within $n^{\log_{b}a-\varepsilon}$ and $n^{\log_{b}a+\varepsilon}$. This leads us to check for $f(n)=\Theta(n^{\log_{b}a}(\log n)^k)$ instead.<p>Now the question remains: <em>why is there a $k+1$ in the total runtime ?</em>. The answer can be a little annoying to prove rigorously but can be explained quite nicely using basic intuition. The total work per recursive iteration is $f(n)$. The number of recursive calls, as established earlier is $\log_{b}n$. This means we’re going to have $f(n)\cdot \log_{b} n$ total work which leads to $n^{\log_{b} a}(\log n)^{k}\cdot \log_{b} n$. And since we can do a change of base to make $\log_{b}n$ into $\log n$ with a constant factor, we can absorb the term into<p>$$ \boxed{T(n)=\Theta(n^{\log_{b}a}(\log n)^{k+1})} $$<p><strong>Example</strong> (Merge Sort)<p>Consider MergeSort with work equation $T(n)=2\cdot T\left( \frac{n}{2} \right)+O(n)$. Here $a=2$ since each call splits the problem into 2 sub problems and $b=2$ since each problem is of size $\frac{n}{2}$. Our baseline becomes $n^{\log_{b}a}=n^{\log_{2}2}=n^1=n$. Then we check $f(n)=O(n)$, therefore we are in case 2 where $f(n)=O(n)=O(n(\log n)^{0})$ therefore<p>$$ T(n)=\Theta(n\log n) $$</section></article></main></div>